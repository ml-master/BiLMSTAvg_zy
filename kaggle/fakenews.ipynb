{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74aeab06",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-28T09:49:42.634197Z",
     "iopub.status.busy": "2024-06-28T09:49:42.633506Z",
     "iopub.status.idle": "2024-06-28T09:49:43.531482Z",
     "shell.execute_reply": "2024-06-28T09:49:43.530482Z"
    },
    "papermill": {
     "duration": 0.905767,
     "end_time": "2024-06-28T09:49:43.533939",
     "exception": false,
     "start_time": "2024-06-28T09:49:42.628172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/word2vec/GoogleNewsUnigrams.txt\n",
      "/kaggle/input/corpus/outFile_pre.tsv\n",
      "/kaggle/input/corpus/outFile_integration.tsv\n",
      "/kaggle/input/corpus/outFile.tsv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86877be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T09:49:43.543598Z",
     "iopub.status.busy": "2024-06-28T09:49:43.543163Z",
     "iopub.status.idle": "2024-06-28T09:49:44.517404Z",
     "shell.execute_reply": "2024-06-28T09:49:44.516517Z"
    },
    "papermill": {
     "duration": 0.981346,
     "end_time": "2024-06-28T09:49:44.519737",
     "exception": false,
     "start_time": "2024-06-28T09:49:43.538391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa5d315",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T09:49:44.530037Z",
     "iopub.status.busy": "2024-06-28T09:49:44.529662Z",
     "iopub.status.idle": "2024-06-28T09:49:58.118003Z",
     "shell.execute_reply": "2024-06-28T09:49:58.116823Z"
    },
    "papermill": {
     "duration": 13.596736,
     "end_time": "2024-06-28T09:49:58.120333",
     "exception": false,
     "start_time": "2024-06-28T09:49:44.523597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-28 09:49:46.818693: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-28 09:49:46.818832: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-28 09:49:46.993640: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7562893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "\n",
    "def readData(dataFile,maxSequenceLength,embeddings):\n",
    "\tlabels=[]\n",
    "\tsources=[]\n",
    "\twordsD={'<PAD>':0,'<UNK>':1}\n",
    "\twordsL=['<PAD>','<UNK>']\n",
    "\tcontents=[]\n",
    "\tnums=[]\n",
    "\tdocumentCVs=[]\n",
    "\ttopicCVs=[]\n",
    "\tsourceCVs=[]\n",
    "\tfileIn = open(dataFile, 'r', encoding='utf-8', errors='ignore')\n",
    "\t# fileIn=open(dataFile,'r')\n",
    "\twhile True:\n",
    "\t\tline=fileIn.readline()\n",
    "\t\tif line=='':\n",
    "\t\t\tbreak\n",
    "\t\tparts=line.strip().split('\\t')\n",
    "\t\tlabel=parts[0]\n",
    "\t\tsource=parts[1]\n",
    "\t\tnum=parts[2]\n",
    "\t\tdocumentCV=parts[3]\n",
    "\t\ttopicCV=parts[4]\n",
    "\t\tsourceCV=parts[5]\n",
    "\t\tcontentWords=parts[6:]\n",
    "\t\tif len(contentWords)>maxSequenceLength:\n",
    "\t\t\tcontentWords=contentWords[0:maxSequenceLength]\n",
    "\t\tcontent=[]\n",
    "\t\tfor contentWord in contentWords:\n",
    "\t\t\tif not contentWord in embeddings:\n",
    "\t\t\t\tcontentWord='<UNK>'\n",
    "\t\t\telif not contentWord in wordsD:\n",
    "\t\t\t\twordsD[contentWord]=len(wordsD)\n",
    "\t\t\t\twordsL.append(contentWord)\n",
    "\t\t\tcontent.append(wordsD[contentWord])\n",
    "\t\tif len(content)<maxSequenceLength:\n",
    "\t\t\tcontent=content+[0]*(maxSequenceLength-len(content))\n",
    "\t\tlabels.append(label)\n",
    "\t\tsources.append(source)\n",
    "\t\tcontents.append(content)\n",
    "\t\tnums.append(num)\n",
    "\t\tdocumentCVs.append(documentCV)\n",
    "\t\ttopicCVs.append(topicCV)\n",
    "\t\tsourceCVs.append(sourceCV)\n",
    "\tfileIn.close()\n",
    "\treturn(labels,sources,nums,wordsL,contents,documentCVs,topicCVs,sourceCVs)\n",
    "\n",
    "\n",
    "def readDocuments(dataFile,maxSequenceLength,embeddings,maxDocumentLength):\n",
    "\t(labels,sources,nums,wordsL,contents,documentCVs,topicCVs,sourceCVs)=readData(dataFile,maxSequenceLength,embeddings)\n",
    "\tlabelsD=[]\n",
    "\tsourcesD=[]\n",
    "\tnumsD=[]\n",
    "\tcontentsD=[]\n",
    "\tdocumentCVsD=[]\n",
    "\ttopicCVsD=[]\n",
    "\tsourceCVsD=[]\n",
    "\tlengthsD=[]\n",
    "\ti=0\n",
    "\tcontentHere=[]\n",
    "\twhile(True):\n",
    "\t\tcontentHere.append(contents[i])\n",
    "\t\tif i==len(nums)-1 or nums[i+1]!=nums[i]:\n",
    "\t\t\tlabelsD.append(labels[i])\n",
    "\t\t\tsourcesD.append(sources[i])\n",
    "\t\t\tnumsD.append(nums[i])\n",
    "\t\t\tdocumentCVsD.append(documentCVs[i])\n",
    "\t\t\ttopicCVsD.append(topicCVs[i])\n",
    "\t\t\tsourceCVsD.append(sourceCVs[i])\n",
    "\t\t\tif len(contentHere)<maxDocumentLength:\n",
    "\t\t\t\tlengthHere=len(contentHere)\n",
    "\t\t\t\tfor j in range(maxDocumentLength-len(contentHere)):\n",
    "\t\t\t\t\tcontentHere.append([0]*maxSequenceLength)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlengthHere=maxDocumentLength\n",
    "\t\t\t\tcontentHere=contentHere[0:maxDocumentLength]\n",
    "\t\t\tlengthsD.append(lengthHere)\n",
    "\t\t\tcontentsD.append(contentHere)\n",
    "\t\t\tcontentHere=[]\n",
    "\t\ti=i+1\n",
    "\t\tif i==len(nums):\n",
    "\t\t\tbreak\n",
    "\treturn(labelsD,sourcesD,numsD,wordsL,contentsD,documentCVsD,topicCVsD,sourceCVsD,lengthsD)\n",
    "\n",
    "\n",
    "def readEmbeddings(vectorsFile):\n",
    "    embeddings={}\n",
    "    with open(vectorsFile, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if len(parts) != 301:\n",
    "                continue\n",
    "            coefs = np.asarray(parts[1:], dtype='float32')\n",
    "            embeddings[parts[0]] = coefs\n",
    "    return embeddings\n",
    "# def readEmbeddings(vectorsFile):\n",
    "# \tembeddings={}\n",
    "# \tfor line in open(vectorsFile):\n",
    "# \t\tparts=line.split()\n",
    "# \t\tif len(parts)!=301:\n",
    "# \t\t\tcontinue\n",
    "# \t\tcoefs=np.asarray(parts[1:],dtype='float32')\n",
    "# \t\tembeddings[parts[0]]=coefs\n",
    "# \treturn(embeddings)\n",
    "\n",
    "def prepareEmbeddings(embeddings,wordsL):\n",
    "\tembeddingMatrix = np.zeros((len(wordsL), 300))\n",
    "\tfor i in range(len(wordsL)):\n",
    "\t\trow=np.zeros(300)\n",
    "\t\tword=wordsL[i]\n",
    "\t\tif word!='<PAD>' and word!='<UNK>':\n",
    "\t\t\trow=embeddings[word]\n",
    "\t\t\trow=row/np.sqrt(sum(row*row))\n",
    "\t\tembeddingMatrix[i]=row\n",
    "\treturn(embeddingMatrix)\n",
    "\n",
    "def lengthToAverageMask(lengthD,maxDocumentLength,binary=False):\n",
    "\tresult=[]\n",
    "\tfor i in range(len(lengthD)):\n",
    "\t\tif binary:\n",
    "\t\t\tmultiplier=1.0\n",
    "\t\telse:\n",
    "\t\t\tmultiplier=maxDocumentLength*1.0/lengthD[i]\n",
    "\t\tvec=[multiplier,multiplier]\n",
    "\t\tvec0=[0.0,0.0]\n",
    "\t\tvector=[vec]*lengthD[i]+[vec0]*(maxDocumentLength-lengthD[i])\n",
    "\t\tresult.append(vector)\n",
    "\treturn(np.array(result))\n",
    "\n",
    "\n",
    "def evaluateD(resultPred,resultTrue):\n",
    "\treturn(np.mean((resultPred[:,1]>0.5)==(resultTrue[:,1]==1)))\n",
    "\n",
    "\n",
    "\n",
    "class Style1():\n",
    "\tdef __init__(self,embeddingMatrix,labelsNum,maxSequenceLength,maxDocumentLength,onGPU):\n",
    "\t\tself.maxSequenceLength=maxSequenceLength\n",
    "\t\tself.maxDocumentLength=maxDocumentLength\n",
    "\t\tself.labelsNum=labelsNum\n",
    "\t\tself.embeddingSize=np.shape(embeddingMatrix)[1]\n",
    "\t\tself.embL=keras.layers.Embedding(np.shape(embeddingMatrix)[0],self.embeddingSize,input_length=maxSequenceLength,weights=[embeddingMatrix],trainable=False)\n",
    "\t\tself.reshape1L=keras.layers.Lambda(self.backend_reshape,output_shape=(maxSequenceLength,self.embeddingSize))\n",
    "\t\tif onGPU:\n",
    "\t\t\tself.LSTMforL=keras.layers.LSTM(units=100,go_backwards=False,return_sequences=False)\n",
    "\t\t\tself.LSTMrevL=keras.layers.LSTM(units=100,go_backwards=True,return_sequences=False)\n",
    "\t\telse:\n",
    "\t\t\tself.LSTMforL=keras.layers.LSTM(units=100,go_backwards=False,return_sequences=False)\n",
    "\t\t\tself.LSTMrevL=keras.layers.LSTM(units=100,go_backwards=True,return_sequences=False)\n",
    "\t\tself.conL=keras.layers.Concatenate(axis=1)\n",
    "\t\tself.denseL=keras.layers.Dense(labelsNum,activation=\"softmax\")\n",
    "\t\tself.reshape2L=keras.layers.Lambda(self.backend_reshape2,output_shape=(maxDocumentLength,labelsNum))\n",
    "\t\tself.multiply=keras.layers.Multiply()\n",
    "\t\tself.poolingL=keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "\tdef backend_reshape(self,x):\n",
    "\t\treturn keras.backend.reshape(x,(-1,self.maxSequenceLength,self.embeddingSize))\n",
    "\n",
    "\tdef backend_reshape2(self,x):\n",
    "\t\treturn keras.backend.reshape(x,(-1,self.maxDocumentLength,self.labelsNum))\n",
    "\n",
    "\tdef getMask(self,lengthD):\n",
    "\t\treturn (lengthToAverageMask(lengthD,self.maxDocumentLength,binary=False))\n",
    "\n",
    "\tdef getModel(self):\n",
    "\t\tinputWords = keras.layers.Input(shape=(self.maxDocumentLength,self.maxSequenceLength,))\n",
    "\t\tinputMask = keras.layers.Input(shape=(self.maxDocumentLength,self.labelsNum,))\n",
    "\t\tx=self.embL(inputWords)\n",
    "\t\tx=self.reshape1L(x)\n",
    "\t\tlstm1=self.LSTMforL(x)\n",
    "\t\tlstm2=self.LSTMrevL(x)\n",
    "\t\tx=self.conL([lstm1,lstm2])\n",
    "\t\tPs=self.denseL(x)\n",
    "\t\tPs=self.reshape2L(Ps)\n",
    "\t\tPs=self.multiply([Ps,inputMask])\n",
    "\t\tPs=self.poolingL(Ps)\n",
    "\t\tmodel=Model(inputs=[inputWords,inputMask], outputs=Ps)\n",
    "\t\treturn(model)\n",
    "\n",
    "\n",
    "dataPath=\"/kaggle/input/corpus\"\n",
    "\n",
    "batch_size=64\n",
    "onGPU=False\n",
    "epochs=10\n",
    "dataset=\"outFile.tsv\"\n",
    "dataset_1=\"outFile_integration.tsv\"\n",
    "\n",
    "# Reading data\n",
    "# exec(open('./functions.py').read())\n",
    "embeddings=readEmbeddings(\"/kaggle/input/word2vec\"+\"/GoogleNewsUnigrams.txt\")\n",
    "MAX_SEQUENCE_LENGTH=120\n",
    "MAX_DOCUMENT_LENGTH=50\n",
    "(labels,sources,nums,wordsL,contents,documentCV,topicCV,sourceCV,lengthD)=readDocuments(dataPath+\"/\"+dataset,MAX_SEQUENCE_LENGTH,embeddings,MAX_DOCUMENT_LENGTH)\n",
    "(labels_1,sources_1,nums_1,wordsL_1,contents_1,documentCV_1,topicCV_1,sourceCV_1,lengthD_1)=readDocuments(dataPath+\"/\"+dataset_1,MAX_SEQUENCE_LENGTH,embeddings,MAX_DOCUMENT_LENGTH)\n",
    "embeddingMatrix=prepareEmbeddings(embeddings,wordsL)\n",
    "embeddingMatrix_1=prepareEmbeddings(embeddings,wordsL_1)\n",
    "\n",
    "# Converting to numpy\n",
    "y=np.asarray(labels,dtype='float32')\n",
    "y_1=np.asarray(labels_1,dtype='float32')\n",
    "allY=np.concatenate((np.expand_dims(1-y,1),np.expand_dims(y,1)),axis=1)\n",
    "allY_1=np.concatenate((np.expand_dims(1-y_1,1),np.expand_dims(y_1,1)),axis=1)\n",
    "allX=np.array(contents)\n",
    "allX_1=np.array(contents_1)\n",
    "\n",
    "# Load models\n",
    "# exec(open('./model.py').read())\n",
    "\n",
    "# Prepare CV scenarios\n",
    "# documentCV=np.asarray(documentCV,dtype='int32')\n",
    "# topicCV=np.asarray(topicCV,dtype='int32')\n",
    "sourceCV=np.asarray(sourceCV,dtype='int32')\n",
    "scenarioCV=sourceCV\n",
    "sourceCV_1=np.asarray(sourceCV_1,dtype='int32')\n",
    "scenarioCV_1=sourceCV_1\n",
    "result=np.array([[-1,-1]]*len(scenarioCV_1),dtype='float32')\n",
    "fold=3\n",
    "# print(\"Evaluating on fold \"+str(fold)+\"...\")\n",
    "whichTest=np.isin(scenarioCV,fold)\n",
    "trainY=allY[~whichTest,]\n",
    "develY=allY[whichTest,]\n",
    "trainX=allX[~whichTest,]\n",
    "develX=allX[whichTest,]\n",
    "style1=Style1(embeddingMatrix,2,MAX_SEQUENCE_LENGTH,MAX_DOCUMENT_LENGTH,onGPU)\n",
    "mask=style1.getMask(lengthD)\n",
    "mask_1=style1.getMask(lengthD_1)\n",
    "trainM=np.array(mask)[~whichTest]\n",
    "develM=np.array(mask)[whichTest]\n",
    "model=style1.getModel()\n",
    "model.compile(optimizer=Adam(),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit([trainX,trainM],trainY, epochs=epochs,validation_data=([develX,develM],develY),batch_size=batch_size)\n",
    "# model.save(\"/kaggle/working\"+\"/my_model.h5\")\n",
    "testX=allX_1\n",
    "testY=allY_1\n",
    "testM=np.array(mask_1)\n",
    "predictions=model.predict([testX,testM])\n",
    "result=predictions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(result)\n",
    "# 创建一个字典，字典的键是列名，值是列的数据\n",
    "df = pd.DataFrame()\n",
    "df['num'] = nums_1\n",
    "df['label'] = labels_1\n",
    "df['predicton'] = result[:, 1]\n",
    "\n",
    "df.to_csv(\"/kaggle/working\"+\"/result_integration_co.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "evaluateD(result,allY_1)\n",
    "print(evaluateD(result,allY_1))\n",
    "\n",
    "# //co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa428f",
   "metadata": {
    "papermill": {
     "duration": 1.211618,
     "end_time": "2024-06-28T11:35:53.209162",
     "exception": false,
     "start_time": "2024-06-28T11:35:51.997544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def readData(dataFile,maxSequenceLength,embeddings):\n",
    "\tlabels=[]\n",
    "\tsources=[]\n",
    "\twordsD={'<PAD>':0,'<UNK>':1}\n",
    "\twordsL=['<PAD>','<UNK>']\n",
    "\tcontents=[]\n",
    "\tnums=[]\n",
    "\tdocumentCVs=[]\n",
    "\ttopicCVs=[]\n",
    "\tsourceCVs=[]\n",
    "\tfileIn = open(dataFile, 'r', encoding='utf-8', errors='ignore')\n",
    "\t# fileIn=open(dataFile,'r')\n",
    "\twhile True:\n",
    "\t\tline=fileIn.readline()\n",
    "\t\tif line=='':\n",
    "\t\t\tbreak\n",
    "\t\tparts=line.strip().split('\\t')\n",
    "\t\tlabel=parts[0]\n",
    "\t\tsource=parts[1]\n",
    "\t\tnum=parts[2]\n",
    "\t\tdocumentCV=parts[3]\n",
    "\t\ttopicCV=parts[4]\n",
    "# \t\tsourceCV=parts[5]\n",
    "\t\tsourceCV=random.randint(0, 5)\n",
    "\t\tcontentWords=parts[6:]\n",
    "\t\tif len(contentWords)>maxSequenceLength:\n",
    "\t\t\tcontentWords=contentWords[0:maxSequenceLength]\n",
    "\t\tcontent=[]\n",
    "\t\tfor contentWord in contentWords:\n",
    "\t\t\tif not contentWord in embeddings:\n",
    "\t\t\t\tcontentWord='<UNK>'\n",
    "\t\t\telif not contentWord in wordsD:\n",
    "\t\t\t\twordsD[contentWord]=len(wordsD)\n",
    "\t\t\t\twordsL.append(contentWord)\n",
    "\t\t\tcontent.append(wordsD[contentWord])\n",
    "\t\tif len(content)<maxSequenceLength:\n",
    "\t\t\tcontent=content+[0]*(maxSequenceLength-len(content))\n",
    "\t\tlabels.append(label)\n",
    "\t\tsources.append(source)\n",
    "\t\tcontents.append(content)\n",
    "\t\tnums.append(num)\n",
    "\t\tdocumentCVs.append(documentCV)\n",
    "\t\ttopicCVs.append(topicCV)\n",
    "\t\tsourceCVs.append(sourceCV)\n",
    "\tfileIn.close()\n",
    "\treturn(labels,sources,nums,wordsL,contents,documentCVs,topicCVs,sourceCVs)\n",
    "\n",
    "\n",
    "def readDocuments(dataFile,maxSequenceLength,embeddings,maxDocumentLength):\n",
    "\t(labels,sources,nums,wordsL,contents,documentCVs,topicCVs,sourceCVs)=readData(dataFile,maxSequenceLength,embeddings)\n",
    "\tlabelsD=[]\n",
    "\tsourcesD=[]\n",
    "\tnumsD=[]\n",
    "\tcontentsD=[]\n",
    "\tdocumentCVsD=[]\n",
    "\ttopicCVsD=[]\n",
    "\tsourceCVsD=[]\n",
    "\tlengthsD=[]\n",
    "\ti=0\n",
    "\tcontentHere=[]\n",
    "\twhile(True):\n",
    "\t\tcontentHere.append(contents[i])\n",
    "\t\tif i==len(nums)-1 or nums[i+1]!=nums[i]:\n",
    "\t\t\tlabelsD.append(labels[i])\n",
    "\t\t\tsourcesD.append(sources[i])\n",
    "\t\t\tnumsD.append(nums[i])\n",
    "\t\t\tdocumentCVsD.append(documentCVs[i])\n",
    "\t\t\ttopicCVsD.append(topicCVs[i])\n",
    "\t\t\tsourceCVsD.append(sourceCVs[i])\n",
    "\t\t\tif len(contentHere)<maxDocumentLength:\n",
    "\t\t\t\tlengthHere=len(contentHere)\n",
    "\t\t\t\tfor j in range(maxDocumentLength-len(contentHere)):\n",
    "\t\t\t\t\tcontentHere.append([0]*maxSequenceLength)\n",
    "\t\t\telse:\n",
    "\t\t\t\tlengthHere=maxDocumentLength\n",
    "\t\t\t\tcontentHere=contentHere[0:maxDocumentLength]\n",
    "\t\t\tlengthsD.append(lengthHere)\n",
    "\t\t\tcontentsD.append(contentHere)\n",
    "\t\t\tcontentHere=[]\n",
    "\t\ti=i+1\n",
    "\t\tif i==len(nums):\n",
    "\t\t\tbreak\n",
    "\treturn(labelsD,sourcesD,numsD,wordsL,contentsD,documentCVsD,topicCVsD,sourceCVsD,lengthsD)\n",
    "\n",
    "\n",
    "def readEmbeddings(vectorsFile):\n",
    "    embeddings={}\n",
    "    with open(vectorsFile, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            if len(parts) != 301:\n",
    "                continue\n",
    "            coefs = np.asarray(parts[1:], dtype='float32')\n",
    "            embeddings[parts[0]] = coefs\n",
    "    return embeddings\n",
    "# def readEmbeddings(vectorsFile):\n",
    "# \tembeddings={}\n",
    "# \tfor line in open(vectorsFile):\n",
    "# \t\tparts=line.split()\n",
    "# \t\tif len(parts)!=301:\n",
    "# \t\t\tcontinue\n",
    "# \t\tcoefs=np.asarray(parts[1:],dtype='float32')\n",
    "# \t\tembeddings[parts[0]]=coefs\n",
    "# \treturn(embeddings)\n",
    "\n",
    "def prepareEmbeddings(embeddings,wordsL):\n",
    "\tembeddingMatrix = np.zeros((len(wordsL), 300))\n",
    "\tfor i in range(len(wordsL)):\n",
    "\t\trow=np.zeros(300)\n",
    "\t\tword=wordsL[i]\n",
    "\t\tif word!='<PAD>' and word!='<UNK>':\n",
    "\t\t\trow=embeddings[word]\n",
    "\t\t\trow=row/np.sqrt(sum(row*row))\n",
    "\t\tembeddingMatrix[i]=row\n",
    "\treturn(embeddingMatrix)\n",
    "\n",
    "def lengthToAverageMask(lengthD,maxDocumentLength,binary=False):\n",
    "\tresult=[]\n",
    "\tfor i in range(len(lengthD)):\n",
    "\t\tif binary:\n",
    "\t\t\tmultiplier=1.0\n",
    "\t\telse:\n",
    "\t\t\tmultiplier=maxDocumentLength*1.0/lengthD[i]\n",
    "\t\tvec=[multiplier,multiplier]\n",
    "\t\tvec0=[0.0,0.0]\n",
    "\t\tvector=[vec]*lengthD[i]+[vec0]*(maxDocumentLength-lengthD[i])\n",
    "\t\tresult.append(vector)\n",
    "\treturn(np.array(result))\n",
    "\n",
    "\n",
    "def evaluateD(resultPred,resultTrue):\n",
    "\treturn(np.mean((resultPred[:,1]>0.5)==(resultTrue[:,1]==1)))\n",
    "\n",
    "\n",
    "\n",
    "class Style1():\n",
    "\tdef __init__(self,embeddingMatrix,labelsNum,maxSequenceLength,maxDocumentLength,onGPU):\n",
    "\t\tself.maxSequenceLength=maxSequenceLength\n",
    "\t\tself.maxDocumentLength=maxDocumentLength\n",
    "\t\tself.labelsNum=labelsNum\n",
    "\t\tself.embeddingSize=np.shape(embeddingMatrix)[1]\n",
    "\t\tself.embL=keras.layers.Embedding(np.shape(embeddingMatrix)[0],self.embeddingSize,input_length=maxSequenceLength,weights=[embeddingMatrix],trainable=False)\n",
    "\t\tself.reshape1L=keras.layers.Lambda(self.backend_reshape,output_shape=(maxSequenceLength,self.embeddingSize))\n",
    "\t\tif onGPU:\n",
    "\t\t\tself.LSTMforL=keras.layers.LSTM(units=100,go_backwards=False,return_sequences=False)\n",
    "\t\t\tself.LSTMrevL=keras.layers.LSTM(units=100,go_backwards=True,return_sequences=False)\n",
    "\t\telse:\n",
    "\t\t\tself.LSTMforL=keras.layers.LSTM(units=100,go_backwards=False,return_sequences=False)\n",
    "\t\t\tself.LSTMrevL=keras.layers.LSTM(units=100,go_backwards=True,return_sequences=False)\n",
    "\t\tself.conL=keras.layers.Concatenate(axis=1)\n",
    "\t\tself.denseL=keras.layers.Dense(labelsNum,activation=\"softmax\")\n",
    "\t\tself.reshape2L=keras.layers.Lambda(self.backend_reshape2,output_shape=(maxDocumentLength,labelsNum))\n",
    "\t\tself.multiply=keras.layers.Multiply()\n",
    "\t\tself.poolingL=keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "\tdef backend_reshape(self,x):\n",
    "\t\treturn keras.backend.reshape(x,(-1,self.maxSequenceLength,self.embeddingSize))\n",
    "\n",
    "\tdef backend_reshape2(self,x):\n",
    "\t\treturn keras.backend.reshape(x,(-1,self.maxDocumentLength,self.labelsNum))\n",
    "\n",
    "\tdef getMask(self,lengthD):\n",
    "\t\treturn (lengthToAverageMask(lengthD,self.maxDocumentLength,binary=False))\n",
    "\n",
    "\tdef getModel(self):\n",
    "\t\tinputWords = keras.layers.Input(shape=(self.maxDocumentLength,self.maxSequenceLength,))\n",
    "\t\tinputMask = keras.layers.Input(shape=(self.maxDocumentLength,self.labelsNum,))\n",
    "\t\tx=self.embL(inputWords)\n",
    "\t\tx=self.reshape1L(x)\n",
    "\t\tlstm1=self.LSTMforL(x)\n",
    "\t\tlstm2=self.LSTMrevL(x)\n",
    "\t\tx=self.conL([lstm1,lstm2])\n",
    "\t\tPs=self.denseL(x)\n",
    "\t\tPs=self.reshape2L(Ps)\n",
    "\t\tPs=self.multiply([Ps,inputMask])\n",
    "\t\tPs=self.poolingL(Ps)\n",
    "\t\tmodel=Model(inputs=[inputWords,inputMask], outputs=Ps)\n",
    "\t\treturn(model)\n",
    "    \n",
    "# \tdef getModel(self):# 消融实验，去掉lstm层\n",
    "# \t\tinputWords = keras.layers.Input(shape=(self.maxDocumentLength,self.maxSequenceLength,))\n",
    "# \t\tinputMask = keras.layers.Input(shape=(self.maxDocumentLength,self.labelsNum,))\n",
    "# \t\tx=self.embL(inputWords)\n",
    "# \t\tx=self.reshape1L(x)\n",
    "# \t\t# 删除 LSTM 层\n",
    "# \t\t# lstm1=self.LSTMforL(x)\n",
    "# \t\t# lstm2=self.LSTMrevL(x)\n",
    "# \t\t# x=self.conL([lstm1,lstm2])\n",
    "# \t\tx = keras.layers.Flatten()(x)  # 将词向量展平\n",
    "# \t\tPs=self.denseL(x)\n",
    "# \t\tPs=self.reshape2L(Ps)\n",
    "# \t\tPs=self.multiply([Ps,inputMask])\n",
    "# \t\tPs=self.poolingL(Ps)\n",
    "# \t\tmodel=Model(inputs=[inputWords,inputMask], outputs=Ps)\n",
    "# \t\treturn(model)\n",
    "\n",
    "\n",
    "\n",
    "dataPath=\"/kaggle/input/corpus\"\n",
    "\n",
    "batch_size=64\n",
    "onGPU=True\n",
    "epochs=10\n",
    "dataset=\"outFile_pre.tsv\"\n",
    "\n",
    "# Reading data\n",
    "# exec(open('./functions.py').read())\n",
    "embeddings=readEmbeddings(\"/kaggle/input/word2vec\"+\"/GoogleNewsUnigrams.txt\")\n",
    "MAX_SEQUENCE_LENGTH=120\n",
    "MAX_DOCUMENT_LENGTH=50\n",
    "(labels,sources,nums,wordsL,contents,documentCV,topicCV,sourceCV,lengthD)=readDocuments(dataPath+\"/\"+dataset,MAX_SEQUENCE_LENGTH,embeddings,MAX_DOCUMENT_LENGTH)\n",
    "embeddingMatrix=prepareEmbeddings(embeddings,wordsL)\n",
    "\n",
    "# Converting to numpy\n",
    "y=np.asarray(labels,dtype='float32')\n",
    "allY=np.concatenate((np.expand_dims(1-y,1),np.expand_dims(y,1)),axis=1)\n",
    "allX=np.array(contents)\n",
    "\n",
    "# Load models\n",
    "# exec(open('./model.py').read())\n",
    "\n",
    "# Prepare CV scenarios\n",
    "documentCV=np.asarray(documentCV,dtype='int32')\n",
    "topicCV=np.asarray(topicCV,dtype='int32')\n",
    "sourceCV=np.asarray(sourceCV,dtype='int32')\n",
    "scenarioCV=sourceCV\n",
    "\n",
    "result=np.array([[-1,-1]]*len(scenarioCV),dtype='float32')\n",
    "for folda in range(max(scenarioCV)):\n",
    "\tfold=folda+1\n",
    "\tprint(\"Evaluating on fold \"+str(fold)+\"...\")\n",
    "\twhichTest=np.isin(scenarioCV,fold)\n",
    "\ttrainY=allY[~whichTest,]\n",
    "\tdevelY=allY[whichTest,]\n",
    "\ttrainX=allX[~whichTest,]\n",
    "\tdevelX=allX[whichTest,]\n",
    "\tstyle1=Style1(embeddingMatrix,2,MAX_SEQUENCE_LENGTH,MAX_DOCUMENT_LENGTH,onGPU)\n",
    "\tmask=style1.getMask(lengthD)\n",
    "\ttrainM=np.array(mask)[~whichTest]\n",
    "\tdevelM=np.array(mask)[whichTest]\n",
    "\tmodel=style1.getModel()\n",
    "\t# model.compile(optimizer=tf.train.AdamOptimizer(),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\tmodel.compile(optimizer=Adam(),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "\tfit=model.fit([trainX,trainM],trainY, epochs=epochs,validation_data=([develX,develM],develY),batch_size=batch_size)\n",
    "\tpredictions=model.predict([develX,develM])\n",
    "\tresult[whichTest,:]=predictions\n",
    "\n",
    "evaluateD(result,allY)\n",
    "print(evaluateD(result,allY))\n",
    "# 创建一个字典，字典的键是列名，值是列的数据\n",
    "df = pd.DataFrame()\n",
    "df['num'] = nums\n",
    "df['label'] = labels\n",
    "df['predicton'] = result[:,1]\n",
    "\n",
    "df.to_csv(\"/kaggle/working/result_pre_style.csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "# //new"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5236941,
     "sourceId": 8726210,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5236349,
     "sourceId": 8790241,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6378.07895,
   "end_time": "2024-06-28T11:35:57.611888",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-28T09:49:39.532938",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
